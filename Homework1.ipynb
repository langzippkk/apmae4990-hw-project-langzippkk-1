{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1 - Introduction to Github, Pandas and Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this introductory homework, we will first get everyone up to speed with how to submit their homework assignments and then cover some topics in supervised learning. The first step\n",
    "is to create a Github account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Set up Github and clone assignment repo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Go to http://www.github.com and create an account.\n",
    "- Fill out this spreadsheet with your Github username and Columbia UNI https://docs.google.com/spreadsheets/d/1KEBDe8H0x_drnqx4ZeMyMUN-6-LrDVy2tBBJ97RZtMY/edit?usp=sharing\n",
    "\n",
    "- Install Git - https://github.com/blog/1510-installing-git-from-github-for-mac. **Make sure to install command line tools. **\n",
    "- Click on this link: https://classroom.github.com/a/Xk990Gdv\n",
    "- Follow the instructions to clone that repo to your local machine.\n",
    "- You should type a command like: \n",
    "\n",
    "```$ git clone https://github.com/Columbia-Intro-Data-Science/apmae4990-hw-project-{your-git-username}```\n",
    "\n",
    "\n",
    "**Next:** Solve the problems directly in this notebook, and then push to the repo above (not to the course repo!). **This is where you will store all of your homework and your final project. **\n",
    "\n",
    "\n",
    "The process should be to create a copy of this notebook, move it into the folder you created above. Then do this:\n",
    "\n",
    "``` $ cd apmae4990-hw-project-{your-git-username} ```\n",
    "\n",
    "``` $ git add myhomeworksolutions.ipynb ``` ( this adds your homework to the files to be pushed )\n",
    "\n",
    "``` $ git status ``` ( this shows which files have been modified and will be part of the commit)\n",
    "\n",
    "``` $ git commit -m \"added my homework 1 solutions\" ``` (commits the files for the push to the repo)\n",
    "\n",
    "``` $ git push origin master $ ``` (pushes the files to the repo)\n",
    "\n",
    "**Writing to the notebook:** To write code in this notebook, you can create a new cell directly below the problem. Either click `Insert -> Insert Cell Below` or type `CTRL-M` **then** `B`.\n",
    "\n",
    "\n",
    "### Very important!!\n",
    "\n",
    "Do not expect to understand every single import statement or piece of code when you first start. You should always think about what your goal is, and try to figure out how to do it yourself. However in many cases it is much easier to simply search stack overflow for the *right* way to do something. You are free to do this and do not need to include any references. This is largely how engineers and data scientists discover things in practice when they are unfamiliar with a topic. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Sales Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will explore our first dataset using `pandas` (for loading and procssing our data) and `sklearn` (for building machine learning models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  radio  newspaper  sales\n",
       "1  230.1   37.8       69.2   22.1\n",
       "2   44.5   39.3       45.1   10.4\n",
       "3   17.2   45.9       69.3    9.3\n",
       "4  151.5   41.3       58.5   18.5\n",
       "5  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import seaborn\n",
    "import numpy.random as nprnd\n",
    "import random\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### What are the features (variables, covariates, all mean the same thing)?\n",
    "\n",
    "- **TV:** advertising dollars spent on TV for a single product in a given market (in thousands of dollars)\n",
    "- **Radio:** advertising dollars spent on Radio\n",
    "- **Newspaper:** advertising dollars spent on Newspaper\n",
    "- **Sales:** Number of 1k units sold. \n",
    "\n",
    "**Goal:** Predict the amount of sales in a given market based on the advertising in TV, Radio and Newspaper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2, Part 0: Plot box plots of the variable ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5 points]** Use df.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2, Part 1: Create scatter plots using `pandas.plotting.scatter_matrix`\n",
    "\n",
    "**[5 points]** Create scatter plots of the advertising dollars spent on TV, Radio and Newspaper to the total Sales dollars gained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the variables seem correlated with one another? Which don't? Explain your answer. \n",
    "\n",
    "Why might these variables be correlated in intuitive terms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probelm 2, Part 2: Predict sales using sklearn\n",
    "\n",
    "- Split data into training and testing subsets.\n",
    "- Train model using LinearRegression() from sklearn.linear_model on training data.\n",
    "- Evaluate using RMSE and R^2 on testing set\n",
    "\n",
    "\n",
    "If you need help, please refer to this example:\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html\n",
    "\n",
    "**Note:** This example does not randomize the test/train split. So please ensure you've done this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points]** a) Set y to be the sales in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points]** b) Set X to be just the features described above in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5 points]** c) Randomly split data into training and testing - 80% training, 20% testing. Make sure your test/train split is random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5 points]** d) Train model on training data, and make predictions on testing data, using our solution from class\n",
    "\n",
    "$$ \\beta = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5 points]** d) Train model on training data, and make predictions on testing data, using `sklearn.linear_model.LinearRegression`. Make sure your answers match (**note**: You may need to pass the argument `fit_intercept=False` to `LinearRegression`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5 points]** e) Evalute the R^2 on training data. Is this good? Bad? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5 points]** f) Interpreting the coefficients of your model (`clf.coef_1`), which form of advertising appears to have the largest impact on sales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5 points]** g) Make a scatter plot of your predictions vs the actual values on the testing data. Does it look like a *good* model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**[10 points]** h) Repeat the steps above but **build a seperate model for each individual feature**, ie. `X = df[col]` where `col` is one of the variables TV, radio and newspaper. Based on this analysis, which feature now appears to have more of an influence on sales? Provide an interpretation of this apparent contradiction. \n",
    "\n",
    "**Hint:** It may be useful to check the correlation matrix using `df.corr()` and to understand how the covariates relate to one another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[10 points]** i) (synnergetic effects) Try plotting the data in three dimensions along with the hyperplane solution to see where the solution you have stops following the linear trend, and see if you can infer\n",
    "a new variable which will help, which is a product of two of our current variables.  More precisely, our previous model has been:\n",
    "\n",
    "$$ y = \\beta_0 x_{0} +  \\beta_0 x_{1} +  \\beta_0 x_{2} + \\epsilon.$$\n",
    "\n",
    "See if you an introduce a new term $$ \\beta_{ij} x_i x_j$$ for some j using your intuition from the previous problems.\n",
    "\n",
    "What is your interpretation of this result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Hint: The code below can be adopted to make your 3d plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x_surf = np.arange(0, 300, 20)\n",
    "y_surf = np.arange(0, 60, 4)\n",
    "x_surf, y_surf = np.meshgrid(x_surf, y_surf)\n",
    "\n",
    "new_x = pd.core.frame.DataFrame({'TV': x_surf.ravel(), 'radio': y_surf.ravel()})\n",
    "# define your regr_1\n",
    "predict_sales = regr_1.predict(new_x)\n",
    "ax.plot_surface(x_surf, y_surf,\n",
    "                predict_sales.reshape(x_surf.shape),\n",
    "                rstride=1,\n",
    "                cstride=1,\n",
    "                color='None',\n",
    "                alpha = 0.4)\n",
    "\n",
    "ax.scatter(X['TV'], X['radio'], y, c='r', marker='o')\n",
    "\n",
    "ax.set_xlabel('TV')\n",
    "ax.set_ylabel(\"Radio\")\n",
    "ax.set_zlabel('sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**[10 points]** h) Does your mixed variable in i) imporve performance? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Regularization and model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had quite a hard time discovering the variables which had true predictive power in the previous section. What if we pnealized\n",
    "the size of the coefficients in order to select the most predictive features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $$\\hat F_{\\alpha}^p(\\beta) : = \\frac{1}{N} \\sum_{i=1}^N (y^{(i)} - \\beta \\cdot \\mathbf{x}^{(i)})^2 + \\alpha \\|\\beta\\|_{L^p}.$$\n",
    "\n",
    "We call $F_{\\alpha}^p$ to be the *Lasso* norm when $p=1$ and *Ridge* norm when $p=2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** [5 points] ** a) First rescale you features to have mean zero and unit variance using\n",
    "\n",
    "\n",
    "```from sklearn import preprocessing\n",
    "X_scaled = preprocessing.scale(X)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** [5 points] ** a) Repeat the regression above, but using `sklearn`'s `Lasso()` method with $\\alpha=0$. Why is the result the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** [10 points] ** b) Choose a range of $\\alpha$ ranging from 0 to 50. Why does the $R^2$ score on test data seem to increase then decrease again?\n",
    "Plot the $R^2$ on test data as you vary $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[10 points]** c) Choose the *best* $\\alpha$ from part b). What is your new score on test data? What are the coefficients for this model? Provide an interpretation for what happened to the coefficients here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[10 points]** d) Why does the above not work as well for `Ridge()`? How do the coefficients vary as you vary $\\alpha$ in the `Ridge` method? \n",
    "\n",
    "How do you explain the difference between this observation and the solution in part c) based on the level sets of $L^p$ for $p=1$ and $p=2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5 points]** Did we need to rescale in part a)? Why would our solution not make sense otherwise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5:  Gradient Descent and the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[10 points]** a) By modifying the learning rate below, show how the convergence takes longer or doesn't converge at all.\n",
    "Can you explain in words or math why this is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, error = 5565.10783448\n",
      "Running...\n",
      "After 10 iterations b = 0.0296393478747, m = 1.47741737555, error = 112.655851815\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy import *\n",
    "\n",
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "    return totalError / float(len(points))\n",
    "\n",
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    return [new_b, new_m]\n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    for i in range(num_iterations):\n",
    "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
    "    return [b, m]\n",
    "\n",
    "def run(num_iterations):\n",
    "    points = genfromtxt(\"../data/data.csv\", delimiter=\",\")\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_m = 0 # initial slope guess\n",
    "    num_iterations = num_iterations\n",
    "    print \"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points))\n",
    "    print \"Running...\"\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    print \"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points))\n",
    "    for i in range(0,len(points)):\n",
    "        plt.scatter(points[i,0],points[i,1])\n",
    "        plt.scatter(points[i,0],m*points[i,0]+b,color='r')\n",
    "\n",
    "run(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to visually discuss convergence rate based on learning rate\n",
    "\n",
    "#for num in range(0,10):\n",
    "#    run(num)\n",
    "#    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) **[10 points]** Plot the error as a function of the number of iterations for various learning rates. Choose the rates\n",
    "so that it tells a story."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6 : Computing to Problem 2 via gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let \n",
    "\n",
    "$$ \\hat F(\\beta) : = \\frac{1}{N} \\sum_{i=1}^N (y^{(i)} - \\beta \\cdot \\mathbf{x}_i)^2 = \\|\\mathbf{y} - \\mathbf{X} \\beta\\|_{L^2}^2, $$\n",
    "\n",
    "where $(\\mathbf{x}_i, y_i)$ are as in Problem 2. \n",
    "\n",
    "\n",
    "**[10 points]** a) Solve $\\nabla \\hat F(\\beta) = 0$ for $\\beta$ in terms of $\\mathbf{X}$ and $\\mathbf{y}$ and show that the solution is\n",
    "\n",
    "$$ \\beta = (\\mathbf X^T\\mathbf X)^{-1} \\mathbf X \\mathbf y. $$\n",
    "\n",
    "*Hint:*\n",
    "Use the product rule for inner products afer rewriting\n",
    "\n",
    "$$\\|\\mathbf{y} - \\mathbf{X} \\beta\\|_{L^2}^2  = \\langle \\mathbf{y} - \\mathbf{X} \\beta, \\; \\mathbf{y} - \\mathbf{X} \\beta \\rangle. $$\n",
    "\n",
    "\n",
    "**[5 points]** b)  What assumption did you have to make to write the solution for $\\beta$ in in part a)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[10 points]** c) Let $\\beta_0 = (0, 0, 0)$ and $\\beta =0.01$. Using your computation of $\\nabla F(\\beta)$ from a), write a function in Python which iterates\n",
    "\n",
    "$$\\beta_t = \\beta_{t-1} - \\nu \\nabla \\hat F(\\beta_{t-1})$$ from $t=0,...,T$.  Are you having trouble with convergence? If so, consider more iterations and a much smaller learning rate. Why do you think we had to do this in this case? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[10 points]** d) After finding a good learning rate, show that as $T$ grows large, your solution to part b) converges to that of Problem 2. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[10 points]** e) Finally consider the Lasso regularized OLS. \n",
    "\n",
    "$$ \\hat F_{\\lambda}(\\beta) : = \\frac{1}{N} \\sum_{i=1}^N (y^{(i)} - \\beta \\cdot \\mathbf{x}_i)^2 = \\|\\mathbf{y} - \\mathbf{X} \\beta\\|_{L^2}^2 + \\lambda \\|\\beta\\|_{L^1}, $$\n",
    "\n",
    "\n",
    "Using the optimal $\\lambda$ found in Problem 4, rewrite your gradient descent algorithm for this reguarlized norm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** [10 points] ** f) Show that you get convergence with a much lower learning rate and less iterations. Can you provide a reason for this in terms of the correlation matrix?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
